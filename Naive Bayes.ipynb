{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea48cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ebb7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = pd.Series(data.target)\n",
    "X=df.drop('target',axis=1)\n",
    "y=df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8929d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8969db",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.\n",
    "In this classifier, the assumption is that *data from each label is drawn from a simple Gaussian distribution*.\n",
    "Imagine that you have the following data:# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd60c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "694da217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368421052631579"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc7576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9714826",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.\n",
    "Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.\n",
    "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e27229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_mn = MultinomialNB()\n",
    "model_mn.fit(X_train, y_train)\n",
    "model_mn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be04c1b",
   "metadata": {},
   "source": [
    "# When to Use Naive Bayes\n",
    "\n",
    "Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages:\n",
    "\n",
    "- They are extremely fast for both training and prediction\n",
    "- They provide straightforward probabilistic prediction\n",
    "- They are often very easily interpretable\n",
    "- They have very few (if any) tunable parameters.\n",
    "\n",
    "These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification. \n",
    "If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem. If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
    "\n",
    "Naive Bayes classifiers tend to perform especially well in one of the following situations:\n",
    "\n",
    "- When the naive assumptions actually match the data (very rare in practice)\n",
    "- For very well-separated categories, when model complexity is less important\n",
    "- For very high-dimensional data, when model complexity is less important\n",
    "\n",
    "The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc91741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
